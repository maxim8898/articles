
# Установка и настройка кластера с использованием OpenMPI
Если Вы вдруг захотели настроить свой кластер для параллельных вычислений, имея при этом несколько свободных машин (ПК), то эта статья для Вас.

Для начала опишу поставленную задачу. Необходимо настроить кластер, состоящий из нескольких узлов, работающих под управлением **Linux**. Все узлы кластера можно разделить на **Консоль** и **Исполнителей**. Так буду называть их я в ходе этой статьи. После настройки необходимо иметь тестовый пример который точно указывал бы нам на работоспособность кластера.

Также опишу стартовые данные. Для уменьшения объема статьи я предполагаю, что у Вас уже установлены системы и сеть находится в рабочем состоянии, т.е. все узлы кластера видят друг друга. Инструкцию по установке, например, Debian не сложно отыскать в интернете, не будем на этом останавливаться. Так же одним из условий является однотипность систем. То есть, желательно устанавливая программу или библиотеку на одном узлу кластера устанавливать их и на другом, причем в ту же дирректорию.

И так, после запуска системы первое что попытаемся сделать - обновить списки пакетов:
```sh
$ sudo apt-get update
```
На этом самом простом этапе иногда возникают проблемы в виде следующей ошибки:
```sh
username is not in the sudoers file. This incident will be reported.
```
В таком случае нужно открыть файл sudoers
```
$ su -
$ nano /etc/sudoers
```
В этом файле нужно найти запись `root ALL=(ALL:ALL) ALL` и добавить ниже точно такую же строку, заменив `root` на имя своего пользователя. Далее сохраняем и закрываем файл (Ctrl+X -> Y -> Enter). Перезапускаем терминал.

Далее необходимо настоить общий каталог для всех узлов. Для этого на **Консоли** создадим в домашней дирректории каталог `mpi`.
```
$ sudo mkdir ~/mpi
$ sudo apt-get install nfs-kernel-server // Установка NFS сервера
```
Пропишем конфиги NFS кому и что мы собираемся шарить. Предположим, что сеть в которой работает кластер имеет следующий вид 192.168.1.0/24, то есть, например **Консоль** имеет IP 192.168.1.1, а **Исполнитель** IP 192.168.1.2. Тогда откроем файл `exports`
```
$ sudo nano /etc/exports
```
Добавим строку 
``/home/mpiuser/mpi 192.168.1.0/24(rw,sync,all_squash,no_subtree_check)`` и сохраним файл
Здесь и далее `mpiuser` - это имя моего пользователя, у Вас он конечно свой.
Далее выполним
```
$ sudo exportfs -a
$ sudo /etc/init.d/nfs-kernel-server restart
```
Далее установим Open MPI
```
$ sudo apt-get install openmpi-bin openmpi-common openmpi-doc
```
Так же нужно убедиться что Существует и правильно задана переменная окружения `LD_LIBRARY_PATH`. Вывести её в терминале можно командой
```
$ echo $LD_LIBRARY_PATH
```
Если вывод пустой, то добавляем её следующим образом:
```
$ sudo nano ~/.bashrc
// Добавим в начало файла следующую строку
export LD_LIBRARY_PATH=/usr/lib
```
Чтобы переменная стала доступной необходимо перезагрузить консоль

Следующим шагом является настройка беспарольного доступа по SSH к узлам кластера. Для этого сначала сгенерируем ключ.
```
// Заходим в каталог ssh
$ cd ~/.ssh
// Если такого каталога ещё нет то создаём командой
$ mkdir ~/.ssh
// И генерируем ключ
$ ssh-keygen -t rsa
// Жмём три раза Enter
```

Теперь переключимся на вторую машину, тобишь **Исполнитель** и проделаем похожие действия
```
// Установим NFS
$ sudo apt-get install nfs-kernel-server
// Откроем файл fstab, чтобы настроить автомонтирование нашего общего каталога
$ sudo nano /etc/fstab
// Добавим в него строку
192.168.1.1:/home/mpiuser/mpi /home/mpiuser/mpi nfs rsize=8192,wsize=8192,rw,auto,exec 0 0
// Одной строкой! Напомню, что 192.168.1.1 - в моём случае Консоль
Далее установим MPI
$ sudo apt-get install openmpi-bin openmpi-common openmpi-doc
// И добавим переменную окружения, как это делали на Консоли
$ sudo nano /home/mpiuser/.bashrc
Строка export LD_LIBRARY_PATH=/usr/lib
```
Теперь время перезагрузить все узлы кластера включая **Консоль**.
Теперьнеобходимо зайти с **Консоли** и отправить SSH ключ на узлы кластера следующим образом
```
$ cd ~/.ssh
$ scp id_rsa.pub mpiuser@192.168.1.2:~/.ssh
// Здесь mpiuser - это пользователь на Исполнителе и его IP соответственно
```
Опять переходим на **Исполнитель** и копируем содержимое нашего ключа в два других файла
```
$ cd ~/.ssh
$ cat id_rsa.pub >> authorized_keys
$ cat id_rsa.pub >> authorized_keys2
```

На этом подготовка для работы Open MPI завершена.
В качестве тестового примера запустим выполнение `python` программы. Поэтому установим `python` и его библиотеку `mpi4py`.
```
$ sudo apt-get install python
$ sudo apt-get install python-mpi4py
```
Далее нужно указать системе на наличие хостов в нашей сети. Для этого откроем файл `hosts`
```
$ sudo nano /etc/hosts
// И добавим в него строку
server1    192.168.1.2
// Здесь server1 - имя хоста (может быть любым словом)
```
В нашем расшаренном каталоге создадим файл хостов, которые будут учавствовать в параллельных вычислениях
```
$ cd ~/mpi
$ sudo nano .mpi_hosts
// И запишем в него следующие строки
    localhost.some.place:1
    server1.some.place:1
```
`localhost` и `server1` - имена хостов, которые должны быть объявлены в файле `hosts`, что мы и сделали несколько ранее. А параметр `.some.place` указывает на количество ядер данного хоста, которые можно использовать.

Теперь в нашей расшаренной папке создадим файл `test.py` с тестовой программой
```
$ nano ~/mpi/test.py
```
Код программы:
```
from mpi4py import MPI
import os
import sys
size = MPI.COMM_WORLD.Get_size()
rank = MPI.COMM_WORLD.Get_rank()
name = MPI.Get_processor_name()
ip = os.popen(‘ip addr show enp0s3’).read().split(‘inet ’)[1].split(‘/’)[0]
sys.stdout.write(
"Hello, World! I am process %d of %d on %s. My ip - %s.\n"
% (rank, size, name, ip))
```

Запускаем её следующим образом
```
$ cd ~/mpi
mpirun -hostfile /home/mpiuser/.mpi_hosts -np 4 python test.py
```